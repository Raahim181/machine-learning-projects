{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Gradient Boosting Models with XGBoost\n",
    "\n",
    "The goal of developing a predictive model is to develop a model that is accurate on unseen data.\n",
    "\n",
    "This can be achieved using statistical techniques where the training dataset is carefully used to estimate the performance of the model on new and unseen data.\n",
    "\n",
    "Here you will discover how you can evaluate the performance of your gradient boosting models with XGBoost in Python.\n",
    "\n",
    "By the end you will know.\n",
    "\n",
    "- How to evaluate the performance of your XGBoost models using train and test datasets?\n",
    "- How to evaluate the performance of your XGBoost models using k-fold cross validation?\n",
    "\n",
    "## Evaluate XGBoost Models With Train and Test Sets\n",
    "\n",
    "The simplest method that we can use to evaluate the performance of a machine learning algorithm is to use different training and testing datasets.\n",
    "\n",
    "We can take our original dataset and split it into two parts. Train the algorithm on the first part, then make predictions on the second part and evaluate the predictions against the expected results.\n",
    "\n",
    "The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing.\n",
    "\n",
    "This algorithm evaluation technique is fast. It is ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem. Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train.\n",
    "\n",
    "A downside of this technique is that it can have a high variance. This means that differences in the training and test dataset can result in meaningful differences in the estimate of model accuracy.\n",
    "\n",
    "We can split the dataset into a train and test set using the **train_test_split()** function from the scikit-learn library. For example, we can split the dataset into a 67% and 33% split for training and test sets as follows:\n",
    "\n",
    "<pre>\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "</pre>\n",
    "\n",
    "The full code listing is provided below using the Pima Indians onset of diabetes [dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes), assumed to be in the current working directory. An XGBoost model with default configuration is fit on the training dataset and evaluated on the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.95%\n"
     ]
    }
   ],
   "source": [
    "# train-test split evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate XGBoost Models With k-Fold Cross Validation\n",
    "\n",
    "Cross validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split.\n",
    "\n",
    "It works by splitting the dataset into k-parts (e.g. k=5 or k=10). Each split of the data is called a fold. The algorithm is trained on k-1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set.\n",
    "\n",
    "After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation.\n",
    "\n",
    "The result is a more reliable estimate of the performance of the algorithm on new data given your test data. It is more accurate because the algorithm is trained and evaluated multiple times on different data.\n",
    "\n",
    "The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest sized datasets in the thousands or tens of thousands of observations, k values of 3, 5 and 10 are common.\n",
    "\n",
    "We can use k-fold cross validation support provided in scikit-learn. First we must create the KFold object specifying the number of folds and the size of the dataset. We can then use this scheme with the specific dataset. The **cross_val_score()** function from scikit-learn allows us to evaluate a model using the cross validation scheme and returns a list of the scores for each model trained on each fold.\n",
    "\n",
    "<pre>\n",
    "kfold = KFold(n=len(X), n_folds=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "</pre>\n",
    "\n",
    "The full code listing for evaluating an XGBoost model with k-fold cross validation is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.69% (7.11%)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "import xgboost\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# CV model\n",
    "model = xgboost.XGBClassifier()\n",
    "kfold = KFold(n=len(X), n_folds=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example summarizes the performance of the default model configuration on the dataset including both the mean and standard deviation classification accuracy.\n",
    "\n",
    "If you have many classes for a classification type predictive modeling problem or the classes are imbalanced (there are a lot more instances for one class than another), it can be a good idea to create stratified folds when performing cross validation.\n",
    "\n",
    "This has the effect of enforcing the same distribution of classes in each fold as in the whole training dataset when performing the cross validation evaluation. The scikit-learn library provides this capability in the StratifiedKFold class.\n",
    "\n",
    "Below is the same example modified to use stratified cross validation to evaluate an XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.95% (5.88%)\n"
     ]
    }
   ],
   "source": [
    "# stratified k-fold cross validation evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "import xgboost\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# CV model\n",
    "model = xgboost.XGBClassifier()\n",
    "kfold = StratifiedKFold(Y, n_folds=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Techniques to Use When\n",
    "\n",
    "- Generally k-fold cross validation is the gold-standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.\n",
    "- Use stratified cross validation to enforce class distributions when there are a large number of classes or an imbalance in instances for each class.\n",
    "- Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\n",
    "The best advice is to experiment and find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions.\n",
    "\n",
    "If in doubt, use 10-fold cross validation for regression problems and stratified 10-fold cross validation on classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
