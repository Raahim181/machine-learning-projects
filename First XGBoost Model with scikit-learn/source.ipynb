{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First XGBoost Model with scikit-learn\n",
    "\n",
    "XGBoost is an implementation of gradient boosted decision trees designed for speed and performance that is dominating  machine learning competitions.\n",
    "\n",
    "For up-to-date instructions for installing XGBoost for Python [see](http://xgboost.readthedocs.io/en/latest/build.html#building-on-osx).\n",
    "\n",
    "For reference, you can review the XGBoost Python [API](http://xgboost.readthedocs.io/en/latest/python/python_api.html) reference.\n",
    "\n",
    "## Problem Description: Predict Onset of Diabetes\n",
    "\n",
    "We are going to use the Pima Indians onset of diabetes [dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes). \n",
    "\n",
    "This dataset is comprised of 8 input variables that describe medical details of patients and one output variable to indicate whether the patient will have an onset of diabetes within 5 years.\n",
    "\n",
    "This is a good dataset for a first XGBoost model because all of the input variables are numeric and the problem is a simple binary classification problem. It is not necessarily a good problem for the XGBoost algorithm because it is a relatively small dataset and an easy problem to model.\n",
    "\n",
    "## Train the XGBoost Model\n",
    "\n",
    "XGBoost provides a wrapper class to allow models to be treated like classifiers or regressors in the scikit-learn framework.\n",
    "\n",
    "This means we can use the full scikit-learn library with XGBoost models.\n",
    "\n",
    "The XGBoost model for classification is called **XGBClassifier**. We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.\n",
    "\n",
    "Parameters for training the model can be passed to the model in the constructor. Here, we use the sensible defaults.\n",
    "\n",
    "You can learn more about the meaning of each parameter and how to configure them on the XGBoost [parameters](http://xgboost.readthedocs.io/en/latest//parameter.html) page.\n",
    "\n",
    "You can see the parameters used in a trained model by printing the model, for example:\n",
    "\n",
    "<pre>\n",
    "print(model)\n",
    "</pre>\n",
    "\n",
    "## Make Predictions with XGBoost Model\n",
    "\n",
    "We can make predictions using the fit model on the test dataset.\n",
    "\n",
    "To make predictions we use the scikit-learn function model.predict().\n",
    "\n",
    "By default, the predictions made by XGBoost are probabilities. Because this is a binary classification problem, each prediction is the probability of the input pattern belonging to the first class. We can easily convert them to binary class values by rounding them to 0 or 1.\n",
    "\n",
    "Now that we have used the fit model to make predictions on new data, we can evaluate the performance of the predictions by comparing them to the expected values. For this we will use the built in accuracy_score() function in scikit-learn.\n",
    "\n",
    "We can tie all of these pieces together, below is the full code listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Accuracy: 77.95%\n"
     ]
    }
   ],
   "source": [
    "# First XGBoost model for Pima Indians dataset\n",
    "import numpy\n",
    "import xgboost\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data\n",
    "dataset = numpy.loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# fit model no training data\n",
    "model = xgboost.XGBClassifier()\n",
    "print model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good accuracy score on this problem, which we would expect, given the capabilities of the model and the modest complexity of the problem."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
